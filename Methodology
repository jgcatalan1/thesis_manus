Materials and methods
System Design and Architecture
This study employed a comprehensive system design approach that integrated machine learning capabilities with mobile application development to deliver an accessible yield prediction tool for rice farmers. The system architecture was designed to function offline, ensuring usability in areas with limited internet connectivity while maintaining computational efficiency on mobile devices. The design framework encompassed data collection, model development, mobile application integration, and user interaction components that worked together to provide accurate yield predictions based on agronomic inputs.
As shown in Fig. 16, the conceptual framework demonstrates the interconnected relationship between farmers as end-users, data collection processes, machine learning algorithms, and mobile application interfaces. This framework serves as the foundation for understanding how agronomic data flows through the system to generate actionable yield predictions that support farmer decision-making in rice production.
 
Fig. 16. Conceptual framework of the study
The system architecture followed a three-tier design pattern, consisting of the presentation layer (mobile user interface), business logic layer (data processing and Random Forest model), and data persistence layer (local storage for farmer inputs and prediction results). This modular approach ensured scalability, maintainability, and efficient resource utilization on mobile devices while providing seamless user experience for farmers with varying levels of technological expertise.
The technology stack integrated Python-based machine learning development with Kotlin-based Android application development, utilizing the m2cgen library for model conversion and ensuring compatibility across the development and deployment phases.  
Development Methodology Framework
This study adopted the Rapid Application Development (RAD) model as the primary software development methodology for creating the GabayPalay mobile application. The RAD model was selected due to its iterative approach, emphasis on user feedback, and capability to accommodate evolving requirements throughout the development process.
As illustrated in Fig. 17, the RAD model consists of four phases: Requirements Planning, User Design, Construction, and Cutover. This methodology provided a systematic framework for developing the mobile application from initial user requirements to final deployment and evaluation. Each phase incorporated continuous testing and validation to ensure that both technical functionality and user experience requirements were met throughout the development lifecycle. The implementation of this methodology required careful attention to research ethics and participant protection, particularly given the involvement of rice farmers in various stages of the development process.
 
Fig. 17. Rapid Application Development Model lifted from Dennis (2023).
Ethical considerations
All participants in this study were thoroughly informed about the nature, objectives, and procedures of the research prior to their involvement. Only individuals aged 18 to 59 years old were included, ensuring that all participants were legally capable of giving informed consent. Participation was completely voluntary, and participants were free to withdraw from the study at any time without penalty or consequence.
No personally identifiable information was collected, and all responses were kept strictly confidential. Data were anonymized during analysis and reporting to safeguard participants’ privacy. The instruments and procedures used in this study were designed to minimize any potential discomfort, risk, or burden to participants.
Prior to any data collection, the informed consent form (see Appendix 1) was provided and explained to each participant, ensuring that they understood their rights and the study’s procedures. Throughout the entire research process, ethical standards were upheld to guarantee the responsible and respectful treatment of all individuals involved.
Phase 1: Requirements Planning
The requirements planning phase constituted the foundational stage of the RAD methodology, focusing on systematic investigation of farming practices, technology usage patterns, and productivity factors to inform requirements identification for the GabayPalay mobile application.
Mixed-method research design
The investigation employed an exploratory sequential mixed-method design (Creswell and Creswell, 2018) which combined qualitative interviews with quantitative surveys to establish comprehensive understanding of rice farming contexts in Hagonoy, Davao del Sur, as illustrated in Fig. 18. 
The qualitative component involved face-to-face interviews with rice farmers to explore current farming practices, technology adoption barriers, and improvement needs for rice productivity. Interview findings were validated through review by a rice farming expert. The quantitative phase employed structured surveys to collect numerical data on farming parameters, yield outcomes, and input management patterns among the target user population.
 
Fig. 18. Exploratory sequential mixed-method design integrating qualitative interviews and quantitative surveys for requirements planning. 
Participant selection and sampling
Purposive sampling was employed for both qualitative and quantitative phases to select rice farmers from Hagonoy, Davao del Sur (Lavrakas, 2011; Palinkas et al., 2015). This non-probability sampling approach was appropriate for the exploratory research objective, enabling deliberate selection of participants with specific characteristics necessary for addressing research questions.
For the qualitative phase, selection criteria emphasized diversity across farm size (hectares), farming experience (years), and technological familiarity (smartphone usage patterns) to capture varied perspectives on farming practices and technology adoption contexts. For the quantitative phase, selection criteria included active rice production during the current season, availability of farm input and yield data from completed harvest, willingness to provide detailed agronomic information, and geographic accessibility within Hagonoy municipality.
Recruitment for both phases was facilitated through the Hagonoy Farmers Multi-Purpose Cooperative (HaFaMuPCo), barangay agricultural technicians, and referrals from initial participants. All participants provided informed consent (Appendix 1) prior to data collection, with full disclosure of research objectives, voluntary participation rights, and confidentiality protections.
Qualitative data collection
Semi-structured interviews were conducted face-to-face at locations convenient to participants with interview sessions lasting 30 to 60 minutes per participant. A guide questionnaire (Appendix 2) structured interviews around three areas. Specifically, the first area covered current farming practices including land preparation, seed selection, fertilization schedules, pest management, irrigation, and harvesting. The second area examined mobile application usage patterns including awareness of agricultural apps, prior experience with digital tools, barriers to adoption, current information sources, and perceptions of digital tools. The third area explored productivity factors including challenges during production cycles, yield variability, input management decisions, desired tools for improvement, and planning approaches.
Quantitative data collection
Structured face-to-face surveys were administered using a standardized questionnaire (Appendix 3). Survey administration occurred at locations convenient to farmers, with sessions lasting 20 to 30 minutes. The survey captured variables in five categories: Pre-planting (land area, variety, seed amount, planting method, preparation, irrigation); Nutrient Management (fertilizer products, quantities, and application timings); Pest Management (indicators for key pests and quantities of pesticides); Environmental (weather-related challenges); and the Outcome (palay yield) aligned with the step-by-step rice production from IRRI Rice Knowledge Bank (n.d.).
Data integration and requirements identification
All interview data was analyzed using thematic analysis to identify common patterns in farming practices, technological needs, and user preferences. Survey data provided quantitative validation of key agronomic variables and usage patterns identified during the qualitative phase. Combining qualitative insights with quantitative validation, directly informed the establishment of the system's functional (what the system should do) and non-functional (how the system should perform) requirements for the GabayPalay application.
Phase 2: Data Processing and Model Development
The data processing and model development phase constituted the core technical component of the RAD methodology, focusing on systematic data organization, preprocessing, and machine learning model implementation for rice yield prediction.
Dataset characteristics and collection
Survey data collected from rice farmers in Hagonoy, Davao del Sur during Phase 1 was compiled and prepared for analysis. Each survey response captured complete production cycle information from pre-planting activities through harvest, including farm characteristics, crop management practices, and yield outcomes.
Feature identification framework
Input features for the yield prediction model were identified based on the requirements planning process in Phase 1, which combined farmer interviews, survey responses, and analysis of current farming practices. Features were organized according to their corresponding activity stage within the rice production process, following established agricultural frameworks from the Step-by-step production - IRRI Rice Knowledge Bank (n.d.). Variables were categorized into three main groups: Pre-planting (land preparation and planting decisions), and Growth Management (crop management and pest control).
This activity-based categorization ensured that the model structure aligned with actual farming workflows and facilitated practical interpretation of results by farmers. Feature selection criteria included practical measurability by farmers, agronomic relevance to rice yield, and compatibility with machine learning regression frameworks.


Data preprocessing pipeline
A systematic data preprocessing pipeline was implemented through a combination of manual data validation in Microsoft Excel and automated Python-based transformations. The preprocessing workflow integrated initial data quality checks with algorithmic nutrient standardization to prepare the dataset for model training.
Data loading and initial quality control. The preprocessing workflow began with data loading, where validated survey data was imported into a pandas DataFrame using Python's data manipulation libraries. Prior to algorithmic processing, initial data quality control procedures were performed manually, including identification and handling of missing values, verification of response completeness, and validation of data entry accuracy. This manual review ensured that only complete and valid survey responses were included in the final dataset.
Feature engineering and binary encoding. Categorical variables were transformed into binary format during the data validation phase to enable their use in numerical modeling frameworks. Rice variety was encoded as binary (1 = Hybrid, 0 = Inbred), planting method as binary (1 = Transplanted, 0 = Direct Seeding), and land preparation methods (manual, tractor, rotavator) as individual binary indicators (1 = Yes, 0 = No). Pest presence variables and irrigation source availability were similarly encoded as binary values. This encoding preserved the categorical nature of the variables while enabling their integration into the Random Forest regression framework.
NPK standardization. NPK standardization constituted a critical preprocessing step specific to agricultural data, implemented through an automated Python script. Fertilizer application entries were mathematically standardized based on the NPK ratio declared on the fertilizer label. Common fertilizer types used by farmers in Hagonoy were mapped to their nutrient compositions: Urea (46-0-0), Ammonium Sulfate (21-0-0), Ammonium Phosphate (16-20-0), Complete (14-14-14), and Potash (0-0-60).
The automated script computed total nutrient content for each fertilizer application timing using the following equations:
"Nitrogen " ("N" )" supplied" (kg)= x × (a/100)	[7]

"Phosphorus " (P)  "supplied as" "P" _"2"  "O" _"5"   (kg)= x × (b/100)	[8]

"Potassium " ("K" )" supplied" as "K" _"2"  "O" (kg)= x × (c/100)	[9]

where x is the amount of fertilizer applied (in kg) and a:b:c are the NPK percentages indicated on the fertilizer label. This method follows the guidelines of the Department of Agriculture-Agricultural Training Institute – Cordillera Administrative Region (2022) for fertilizer nutrient calculation.
The script iteratively processed each fertilizer application record, multiplying the amount applied by the respective NPK percentages (divided by 100 to convert from percentage), and aggregated the total nitrogen, phosphorus as P₂O₅, and potassium as K₂O supplied at each application timing (first, second, and third). Null values and zero amounts were excluded from the summation to ensure accurate nutrient totals. Original fertilizer-specific columns were removed after NPK computation to avoid data redundancy and maintain a concise feature set.
Feature naming standardization. All agronomic and management variables were systematically renamed using standardized terms to ensure consistency throughout the data processing and modeling pipeline. This naming convention facilitated clear variable tracking from data preprocessing through model training, validation, and deployment. The processed dataset was exported for subsequent model training procedures.
Data partitioning. The preprocessed dataset was partitioned into training (70%) and testing (30%) sets using scikit-learn's train_test_split function with a fixed random state to ensure reproducibility. This split was performed during the model training phase to maintain clear separation between data preprocessing and model development stages. The training set was used for hyperparameter tuning and model training, while the test set remained held-out for unbiased evaluation of final model performance on unseen data. 
All preprocessing steps were implemented through modular Python scripts provided in Appendix 4, and documented manual procedures to ensure transparency, reproducibility, and adaptability throughout the data preparation workflow. The complete preprocessing codebase was version-controlled and documented to facilitate validation and future modifications.
Machine learning pipeline architecture
Building upon the best practices and pipeline frameworks identified in the RRL, this study adapts a modular machine learning (ML) pipeline architecture for yield prediction, as illustrated in Fig. 13. While the agronomic variables framework defines the domain structure and timing of input variables, the ML pipeline establishes the technical workflow for transforming these inputs into actionable yield predictions.
To address the specific requirements of this study, the pipeline was tailored to ensure consistent handling and traceability of variables from survey collection to final yield prediction. Key components included data ingestion modules that standardized input formats, preprocessing transformations that prepared features for modeling, model training procedures that optimized prediction performance, validation routines that assessed generalization capability, and deployment mechanisms that converted trained models for mobile integration.
The pipeline architecture emphasized modularity and reproducibility, with each component designed as an independent, testable unit. This design facilitated iterative development, systematic debugging, and seamless integration with the mobile application architecture detailed in Phase 3. Data flow through the pipeline maintained clear provenance, enabling validation of intermediate outputs and troubleshooting of processing errors.
Random forest implementation and optimization
The Random Forest regression model was implemented using Python’s scikit-learn library (Pedregosa et al., 2011), following the ensemble learning approach described by Breiman (2001). The model utilized bootstrap aggregating (bagging) and feature randomization techniques to improve prediction accuracy and reduce overfitting compared to individual decision trees.
Model formulation. The Random Forest algorithm operates through a four-stage process: bootstrap sampling, recursive tree construction with feature randomization, individual tree prediction, and ensemble aggregation. Following Breiman (2001), for each tree t in the ensemble (t=1,2,…,T), a bootstrap sample D_t is drawn with replacement from the training dataset D_train of size N (equation [10]): 
D_t={(X_i,y_i ):i=S_t}	[10]

where S_t is a random sample of indices drawn with replacement from {1,2,…,N}, such that |S_t |=N. This bootstrap sampling procedure, a form of bagging (Breiman, 2001), ensures that each tree is trained on different subset of the data, introducing diversity across the ensemble. Approximately 63.2% of unique observations appear in each bootstrap sample, with the remaining ~36.8% forming out-of-bag samples for unbiased error estimation.
As described by (James et al., 2023b), each tree h_t is is constructed through recursive binary splitting. At each internal node, a random subset of m features is selected from the total p features:
F_"node" ⊂{X_1,X_2,…,X_27 }         "where        " |F_"node"  |=m	[11]

The value of m is determined by the max_features hyperparameter, and among the m candidate features, the algorithm selects the feature X_j and threshold θ that minimize the mean squared error (MSE):
(j^*,θ^* )=(arg min)┬(j∈F_("node" ,θ) )⁡[〖MSE〗_left (j,θ)+〖MSE〗_right (j,θ)]	[12]

where MSE is defined as:
"MSE"=1/n_"node"   ∑_(i∈"node" )▒(y_i-y ̅_"node"  )^2 	[13]

With n_"node"  denoting the number of samples in the node and y ̅_"node"  representing their mean yield. This splitting process continues recursively until stopping criteria are met (max_depth, min_samples_split, min_samples_leaf).
For a new input vector X=(X_1,X_2,…X_n) representing farm characteristics, each tree produces a prediction by traversing to a terminal leaf and returning the mean yield of training samples in that leaf:
h_t (X)=y ̅_"leaf"  (X)	[14]

The final Random Forest prediction aggregates individual tree predictions through arithmetic averaging (Breiman, 2001; Hastie et al., 2009; James et al., 2023b) (Equation [15]):
y ̂_RF (X)=1/T ∑_(t=1)^( T)▒〖h_t (X) 〗	[15]

where T represents the total number of decision trees, h_t (X)  denotes the prediction from the t-th tree for input X, and  y ̂_RF (X) represents the estimated rice yield in kilograms. his averaging procedure reduces prediction variance while maintaining low bias, a key advantage of ensemble methods.
The model was implemented using scikit-learn’s RandomForestRegressor with default parameters for bootstrap sampling (bootstrap=True) and splitting criterion (criterion=’squared error’), which implement the procedures described in Equations [10]-[15]. Random seed was fixed at 42 for reproducibility.
Hyperparameter optimization. The hyperparameter search space was defined based on established Random Forest tuning guidelines (Breiman, 2001; James et al., 2023b) and and computational constraints for mobile deployment. 
The n_estimators parameter (T in Equation [15]) was tested across [100, 200, 300, 400] trees. James et al. (2023b) observed that Random Forest performance typically stabilizes after 100-200 trees, with diminishing returns beyond this range. The max_depth parameter was tested as [None, 10, 20] to balance model expressiveness with overfitting risk, particularly important given the limited training data relative to feature dimensionality. The min_samples_split [2, 4, 6] and min_samples_leaf [1, 2, 3] parameters control tree complexity and prediction smoothness, with higher values serving as regularization mechanisms to prevent overfitting (Hastie et al., 2009).
The max features parameter implements the feature randomization described in Equation [11], the key innovation distinguishing Random Forests from simple bagging (Breiman, 2001). Breiman (2001) recommended m = √p features per split for regression, implemented through max_features = ‘sqrt’. The alternative ‘log2’ option produces similar behavior. Both were tested to decorrelate trees and improve ensemble performance compared to the default behavior of using all features at each split.
The complete hyperparameter search space, summarized in Table 5, encompassed 216 combinations (4 ×3×3×3×2) Grid search with 3-fold cross-validation systematically evaluated all combinations to identify the optimal configuration that maximized mean R² while maintaining computational feasibility (Bergstra and Bengio, 2012).
Table 5. Hyperparameter search space for Random Forest grid search optimization.
Hyperparameter	Values Tested
n_estimators	[100, 200, 300, 400]
max_depth	[None, 10, 20]
min_samples_split	[2, 4, 6]
min_samples_leaf	[1, 2, 3]
max_features	['sqrt', 'log2']
Total combinations	216
(butangan unta ni og another column for literature support)
Model training and validation workflow
The model training and validation workflow, shown in Fig. 20, established a systematic process for developing, testing, and validating the Random Forest regression model while maintaining generalization performance and preventing overfitting.
 
Fig. 19. Model training and validation workflow. The trained model was exported into standalone Kotlin code compatible with Android applications.
The training phase employed grid search with three-fold cross-validation (k=3) to systematically evaluate all hyperparameter combinations. Three-fold cross-validation was selected to balance the need for robust performance estimation with the constraints of limited dataset size typical of localized agricultural studies. For small agricultural datasets (n<100) with a 70/30 train-test split, 3-fold cross-validation provides a reasonable balance between training set size and validation set size for performance estimation, as recommended by Kuhn and Johnson (2013).
Performance evaluation utilized four standard regression metrics to comprehensively assess model accuracy and reliability. Mean Square Error (MSE) calculated the average squared difference between actual and predicted values (equation [16]):
"MSE"=1/n ∑_(i=1)^n▒〖(y_i-y ̂_i )^2  〗	[16]

Root Mean Square Error (RMSE) provided error measurement in the same units as the dependent variable (equation [17]):
"RMSE"=√("MSE" )	[17]

Mean Absolute Error (MAE) calculated the average absolute differences between actual and predicted values, offering interpretation advantages and reduced sensitivity to outliers (equation [18]):
"MAE"=1/n ∑_(i=1)^n▒〖|y_i-y ̂_i |  〗	[18]

The coefficient of determination (R²) evaluated the proportion of variance in actual yield explained by the model predictions (equation [19]):
R^2=1-(∑_(i=1)^n▒〖(y_i-y ̂_i )^2  〗)/(∑_(i=1)^n▒〖(y_i-y ̅ )^2  〗)	[19]

where n represents the number of samples, y_i denotes actual yield,  y ̂_i represents predicted yield, and  y ̅ indicates the mean actual yield (Kuhn and Johnson, 2013).
Following hyperparameter selection, the final Random Forest model was trained on the complete training set using the optimal configuration identified through grid search. This best model was then evaluated on the held-out test set to assess generalization performance on unseen data. Test set metrics provided an unbiased estimate of model performance for real-world deployment scenarios.
Model persistence and deployment preparation completed the workflow. The trained model was exported using the m2cgen library to convert the scikit-learn Random Forest implementation into standalone Kotlin code compatible with Android applications. This conversion enabled offline yield prediction functionality within the GabayPalay mobile application without requiring external dependencies or internet connectivity. The generated Kotlin code, along with feature importance utilities and prediction interfaces, was integrated into the mobile application architecture detailed in Phase 3. The complete model training and export code is provided in Appendix 5. 
Phase 3: User Design and System Architecture
The user design and system architecture phase focused on translating functional requirements and model specifications into a comprehensive mobile application design. This phase established the design methodology, architectural principles, and user interface development approach that guided the subsequent construction and implementation phase. 
Design methodology framework
The design phase employed a user-centered design approach, integrating requirements identified in Phase 1 with technical considerations from Phase 2 to create a mobile application tailored to rice farmers' needs and technological contexts. Design activities followed iterative cycles of specification, visualization, and refinement to ensure alignment with user requirements and technical feasibility, consistent with the Rapid Application Development methodology (Dennis, 2023).
Design artifacts were developed using standard software engineering visualization tools and techniques, including architectural diagrams, data flow diagrams, control flow diagrams, and interface mockups. These served multiple purposes: communicating design intent among development team members, validating design decisions against requirements, and providing blueprints for implementation.
Architectural design approach
The mobile application architecture was designed following established best practices in Android development (Android Developers, 2025), emphasizing modularity, maintainability, and offline functionality. A layered architectural pattern was selected to separate concerns and enable independent development and testing of components.
The architectural design process began by analyzing functional requirements and technical constraints identified in earlier phases. Key architectural drivers included: (1) offline yield prediction capability without internet connectivity, (2) local data storage for historical records, (3) integration of the converted Random Forest model, and (4) intuitive user interface for farmers with varying technological backgrounds (Angchay et al., 2024).
Design decisions prioritized simplicity and reliability over feature complexity, recognizing the target users' limited experience with agricultural mobile applications. Similar considerations have been documented in studies of digital technology adoption among Philippine farmers (Philippine Institute for Development Studies et al., 2023). The architecture was structured to isolate business logic from presentation concerns, facilitating future enhancements and maintenance.
System data model and information flow
Data flow diagrams were developed to visualize and specify how information would traverse the system from user input through processing to final output. The design process followed a top-down approach, beginning with high-level context diagrams and progressively decomposing into detailed process specifications, following standard systems analysis and design practices.
The design methodology employed standard Data Flow Diagram (DFD) notation, distinguishing between external entities (farmers), processes (data transformations), data stores (local database), and data flows (information movement). This systematic approach ensured comprehensive coverage of all data processing requirements while maintaining clear boundaries between system components.
Design sessions involved iterative refinement, where initial diagrams were reviewed against functional requirements and adjusted to ensure completeness, consistency, and feasibility. Special attention was given to data validation points, error handling pathways, and offline data persistence mechanisms.
Application logic and user workflow
The control flow design established the sequence of user interactions and system responses throughout the application lifecycle. Design activities focused on mapping user workflows identified during requirements planning to specific application screens and navigation pathways. Control flow diagrams were developed using standard flowchart notation, representing user actions, decision points, validation checks, and system processes. The design approach emphasized linear, intuitive workflows that minimize cognitive load and reduce potential for user error.
Navigation design followed mobile application best practices, ensuring clear entry points, obvious progression through multi-step processes, and consistent mechanisms for recovery error and backward navigation. The design accommodated both primary use cases (yield prediction) and secondary functions (historical data review). 
User interface design and implementation
User interface design prioritized accessibility, simplicity, and effectiveness for rice farmers with diverse technological backgrounds and experience levels. Design principles emphasized intuitive navigation, clear visual hierarchy, and minimal cognitive load to reduce learning requirements and maximize adoption potential.
The interface design accommodated findings from Angchay et al. (2024) regarding technology adoption challenges among farmers in Davao del Sur, including digital literacy gaps and device compatibility issues. Design solutions addressed these constraints through simplified interaction patterns, clear visual feedback, and robust error prevention mechanisms.
Input interface design utilized familiar form-based layouts with clear labeling, appropriate input types, and contextual guidance. Numeric inputs incorporated validation mechanisms to prevent common entry errors while providing immediate feedback regarding acceptable value ranges. Progressive disclosure techniques minimize interface complexity while maintaining access to comprehensive functionality.
The results presentation emphasized clear visualization of yield predictions with contextual information to support farmer decision-making. Output displays included numerical predictions, graphical representations, and textual explanations to accommodate different user preferences and comprehension levels.
Navigation design employed consistent patterns throughout the application, with clear pathways between primary functions and obvious methods for returning to previous states. The interface maintained visual consistency with established Android design guidelines while incorporating agricultural imagery and terminology familiar to the target user population.
System integration and compatibility
System integration procedures ensured seamless operation of application components while maintaining compatibility with diverse Android device configurations used by the target farmer population. Integration testing validated data flow integrity, model performance consistency, and user interface responsiveness across different operational scenarios.
Compatibility requirements addressed the minimum device specifications identified during requirements planning. Performance optimization ensured responsive operation within these constraints while maintaining prediction accuracy and user experience quality.
Offline functionality implementation required careful integration of the converted Random Forest model with local data storage and processing capabilities. Testing procedures validated prediction consistency between the original Python implementation and the deployed Kotlin version to ensure model reliability in the mobile environment.
Security considerations addressed data protection requirements for farmer information and prediction results stored locally on devices. Implementation included data encoding for stored information, secure data validation processes, and privacy protection mechanisms aligned with ethical research standards established in Phase 1.
Phase 4: Construction and Implementation
The construction and implementation phase represented the result of the RAD methodology, transforming design specifications and architectural plans into a functional mobile application. This phase focused on systematic development, rigorous testing, and seamless integration of all system components to deliver the GabayPalay mobile application for rice yield prediction.
Development environment and technology stack
The development environment was established using Android Studio as the primary integrated development environment (IDE), providing comprehensive tools for Android application development, debugging, and testing. The technology stack was selected to ensure optimal performance, maintainability, and compatibility with the target device specifications identified during requirements planning.
The core programming language utilized Kotlin, chosen for its modern syntax, null safety features, and seamless interoperability with existing Android libraries. Kotlin's concise syntax and enhanced safety features reduced development time while improving code reliability and maintainability throughout the implementation process.
Development was carried out using Android Studio and the native SDK, targeting API level 26 (Android 8.0 Oreo) and above. This configuration was selected to fulfill the minimum device requirements specified in the non-functional requirements section, ensuring compatibility and reliable performance on devices accessible to the intended users. The application was optimized and tested for operation on devices with at least 2GB RAM and 16GB internal storage and included support for offline usage to accommodate rural connectivity constraints.
Database implementation utilized SQLite for local data storage, providing reliable offline functionality without requiring internet connectivity. SQLite's lightweight architecture and robust data management capabilities aligned with the application's requirement for independent operation in areas with limited network infrastructure.
User interface development was implemented using XML layouts for the front-end, ensuring consistency with Android UI guidelines and accessibility for users with varying technological backgrounds. The UI framework emphasized simplicity and clarity, addressing the usability requirements identified during user research activities.
Version control systems employed Git with GitHub repositories to maintain code integrity, track development progress, and facilitate collaborative development processes. Automated backup procedures ensured code security and enabled rollback capabilities during development iterations.
Model integration process
The model integration process began with training and optimizing a Random Forest regression model in Python using the scikit-learn library. After selecting the best-performing model through grid search and cross-validation, the model was serialized and evaluated to ensure reliable prediction accuracy and consistent performance across all data samples. Feature importance values were also extracted to support interpretability in subsequent application development.
To enable offline predictions on Android devices, the m2cgen library was used to automatically convert the trained model into Kotlin code. The generated code and feature importance utilities were integrated into the mobile application, with thorough validation to confirm consistency between Python and Kotlin predictions. Additional performance testing and error handling were conducted to guarantee stability and reliability even on resource-constrained devices.
Database implementation
Database implementation established a comprehensive local storage system using SQLite to support offline functionality and historical data management. The database schema was designed to accommodate agronomic input variables, prediction results, and application metadata while maintaining data integrity and query efficiency.
The main data tables included records of input variables for each prediction session, farm characteristics such as land area and location details, and prediction history storing yield estimates with associated confidence measures and timestamps. Data validation procedures ensured input consistency and completeness before database storage, helping maintain data reliability throughout the application lifecycle.
Indexing strategies optimized query performance for common operations including historical data retrieval, trend analysis, and report generation. Database optimization addresses storage efficiency concerns while maintaining rapid access to frequently requested information.
Data security implementation included local encryption for sensitive information and prediction results. Security measures addressed privacy protection requirements established during the ethical considerations phase while ensuring data accessibility for legitimate application functions. Backup and recovery procedures provided data protection against device failures, application errors, and user mistakes. Automated backup mechanisms preserved critical information while enabling data restoration and migration capabilities for device upgrades or replacements.
Unit testing and integration testing
Comprehensive testing procedures ensured the reliability, functionality, and performance of the application across different scenarios and device types. Both automated and manual testing approaches were used to validate input validation, prediction algorithms, database operations, and user interface elements. Model outputs in Kotlin were thoroughly checked for consistency against the original Python implementation. Database and UI testing covered data integrity, query efficiency, and responsiveness on various Android devices. Integration and performance tests confirmed smooth operation, resource usage, and stability under typical and intensive conditions.
Error logging and monitoring systems captured application performance data, user interaction patterns, and technical issues throughout the testing phases. This information guided iterative improvements and optimization efforts during development. Quality assurance procedures ensured that all identified issues were systematically addressed prior to deployment.
Alpha testing was conducted internally to verify system functionality, resolve operational errors, and assess consistency of model outputs with Python environment results. This phase identified and addressed technical issues before user evaluation activities.
Beta testing involved selected farmers from the target population in Hagonoy, Davao del Sur, providing real-world validation of application usability and functionality. Beta testing protocols included structured feedback collection using the System Usability Scale (SUS) and Technology Acceptance Model (TAM) instruments to evaluate user experience and acceptance levels.
Phase 5: Cutover and Validation
The cutover and validation phase represented the final stage of the RAD methodology, focusing on comprehensive testing, user acceptance evaluation, and systematic validation of the GabayPalay mobile application. This phase ensured that the developed application met all functional requirements, usability standards, and user expectations before deployment to the target farmer population in Hagonoy, Davao del Sur.
Testing framework overview
A comprehensive testing framework was established to ensure the GabayPalay application's functionality, performance, and usability across multiple evaluation dimensions. This framework integrated technical validation procedures with user-centered evaluation methods, providing a systematic approach to assessing the application's readiness for deployment.
The framework covered several validation layers, including functional testing for feature completeness and accuracy, performance testing for system responsiveness and resource usage, compatibility testing across target device configurations, and usability testing to evaluate the user experience and interface effectiveness.
Technical validation included regression testing to maintain consistency with the original Python implementation, stress testing under intensive usage scenarios, and security testing to ensure data protection and privacy. User validation incorporated structured feedback collection through standardized instruments, observational studies of user interactions, and evaluation of learning curves and adoption barriers. Both quantitative metrics and qualitative assessments were considered.
Throughout the evaluation process, quality assurance protocols ensured consistency in testing methodologies and systematic documentation of results for analysis and improvement planning. Error logging and monitoring systems captured application performance data, user interaction patterns, and technical issues, guiding iterative refinements and optimization efforts. The framework supported ongoing refinement based on testing outcomes while maintaining rigorous validation standards.
User acceptance testing framework
The user acceptance testing framework established a structured approach for evaluating the GabayPalay application's usability, functionality, and user satisfaction among the target farmer population. This process bridged the transition from technical validation to real-world assessment with actual end users.
Testing began with controlled sessions where individual farmers interacted with the application under guided conditions. These sessions enabled observation of user behaviors, identification of interaction difficulties, and assessment of learning needs for successful adoption.
Task-based scenarios were designed to reflect typical farming decision-making situations, guiding users through complete workflow cycles from data input to prediction interpretation and decision-making support. The framework addressed diverse user backgrounds and technological experience, accommodating farmers with varying digital literacy while maintaining consistent evaluation standards.
Feedback was collected using both quantitative instruments and qualitative interviews to capture comprehensive user perspectives on utility, usability, and adoption potential. The framework enabled systematic analysis of user acceptance factors and identified opportunities for improvement.
System Usability Scale assessment
The System Usability Scale (SUS), based on Brooke (1996), was used to quantitatively measure the usability of the GabayPalay application. The assessment was conducted using the ten-item questionnaire (see Appendix 7), rated on a 5-point Likert scale, following standard scoring procedures to produce final scores from 0 to 100. Participants included rice farmers and individuals with relevant technological experience.
SUS scores were interpreted using established benchmarks (Sauro, 2016): scores above 70 indicated acceptable usability, above 80 signified good usability, and above 90 reflected excellent usability. These thresholds provided clear standards for evaluating the application's user experience. The analysis included calculation of mean scores, standard deviations, and confidence intervals to characterize overall performance. Additionally, item-level assessment was conducted to identify specific areas for improvement and guide targeted refinements in the application's design.
Technology Acceptance Model evaluation
The Technology Acceptance Model (TAM), from Davis (1989), assessed user acceptance in terms of perceived usefulness, ease of use, and behavioral intention. The evaluation used a structured questionnaire (see Appendix 8) to capture user perceptions of the application's practical benefits, interface clarity, and willingness to adopt.
Descriptive statistics summarized TAM responses, with correlation analysis exploring links between usefulness, ease of use, and adoption intention. Qualitative feedback provided additional context for user acceptance and highlighted features impacting evaluations. Comparative analysis identified patterns across user backgrounds and informed recommendations for support and training.
Combined SUS and TAM results offered a comprehensive picture of the GabayPalay application's usability and acceptance among rice farmers, guiding final refinements and deployment strategies to enhance adoption and long-term success.
